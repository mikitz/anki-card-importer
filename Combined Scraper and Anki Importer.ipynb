{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有道词典 definitions \n",
    "defs = driver.find_element_by_xpath('//*[@id=\"phrsListTab\"]/div[2]/ul/p').text\n",
    "# LINE Dict. definitions\n",
    "defs = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[3]/ol').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import re\n",
    "from io import StringIO\n",
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import playsound\n",
    "from numpy import random\n",
    "from datetime import datetime\n",
    "from langdetect import detect\n",
    "import winsound\n",
    "from playsound import playsound\n",
    "from gtts import gTTS\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading Today\n"
     ]
    }
   ],
   "source": [
    "# Do I read or listen today for new vocab?\n",
    "file = open('what_do.txt', mode = 'r')\n",
    "what_do = file.read()\n",
    "file.close()\n",
    "if what_do == 'listening':\n",
    "    print(\"Listening Today\")\n",
    "    f = open('what_do.txt', 'w+')\n",
    "    f.write('reading')\n",
    "    f.close()\n",
    "elif what_do == 'reading':\n",
    "    print(\"Reading Today\")\n",
    "    f = open('what_do.txt', 'w+')\n",
    "    f.write('listening')\n",
    "    f.close()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------中文--------\nSeries([], Name: FOREIGN, dtype: object)\n\n--------한국어--------\nSeries([], Name: FOREIGN, dtype: object)\n\n--------日本語--------\nSeries([], Name: FOREIGN, dtype: object)\n\n"
     ]
    }
   ],
   "source": [
    "# What words do I write sentences for today?\n",
    "languages = ['中文', '한국어', '日本語']\n",
    "sentenceDate = datetime.now().date()\n",
    "for i in languages:\n",
    "    # Read the Excel files into DataFrames\n",
    "    wordsDF = pd.read_excel(r'Databases\\{}_words.xlsx'.format(i), sheet_name = 'words', header = 0)\n",
    "    sentencesDF =  pd.read_excel(r'Databases\\{}_sentences.xlsx'.format(i), sheet_name = 'sentences', header = 0)\n",
    "    # Read the Chinese Databases into DataFrames\n",
    "    words = pd.read_excel(r'Databases\\{}_words.xlsx'.format(i), sheet_name = 'words', header = 0)\n",
    "    # Define the date that we want to retreive\n",
    "    desired_date = (sentenceDate - timedelta(days = 14)).strftime(\"%Y-%m-%d\")\n",
    "    # Retrieve the words from the specified date\n",
    "    write = wordsDF['FOREIGN'].where(words['DATE'] == desired_date)\n",
    "    # Drop NAN rows\n",
    "    write = write.dropna().reset_index(drop = True)\n",
    "    # Display the words with which to write sentences\n",
    "    print(\"--------{}--------\\n{}\\n\".format(i, write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing 11 中文 words for scraping...\n",
      "Elapsed time: (奉行) 0h 00m 09s for 1 words (10 remaining)\n",
      "Elapsed time: (吝啬) 0h 00m 14s for 2 words (9 remaining)\n",
      "Elapsed time: (泵) 0h 00m 19s for 3 words (8 remaining)\n",
      "Elapsed time: (引擎) 0h 00m 24s for 4 words (7 remaining)\n",
      "Elapsed time: (往复) 0h 00m 29s for 5 words (6 remaining)\n",
      "Elapsed time: (螺旋桨) 0h 00m 33s for 6 words (5 remaining)\n",
      "Elapsed time: (胡言乱语) 0h 00m 38s for 7 words (4 remaining)\n",
      "Elapsed time: (谣言) 0h 00m 43s for 8 words (3 remaining)\n",
      "Elapsed time: (暴力) 0h 00m 48s for 9 words (2 remaining)\n",
      "Elapsed time: (蔓延) 0h 00m 53s for 10 words (1 remaining)\n",
      "Elapsed time: (遗漏) 0h 00m 58s for 11 words (0 remaining)\n",
      "Processing 11 中文 Vocabulary...\n",
      "0h 01m 02s has elapsed for 1 Vocab words of 11 (10 remaining).\n",
      "0h 01m 04s has elapsed for 2 Vocab words of 11 (9 remaining).\n",
      "0h 01m 07s has elapsed for 3 Vocab words of 11 (8 remaining).\n",
      "0h 01m 09s has elapsed for 4 Vocab words of 11 (7 remaining).\n",
      "0h 01m 12s has elapsed for 5 Vocab words of 11 (6 remaining).\n",
      "0h 01m 14s has elapsed for 6 Vocab words of 11 (5 remaining).\n",
      "0h 01m 17s has elapsed for 7 Vocab words of 11 (4 remaining).\n",
      "0h 01m 19s has elapsed for 8 Vocab words of 11 (3 remaining).\n",
      "0h 01m 22s has elapsed for 9 Vocab words of 11 (2 remaining).\n",
      "0h 01m 24s has elapsed for 10 Vocab words of 11 (1 remaining).\n",
      "0h 01m 27s has elapsed for 11 Vocab words of 11 (0 remaining).\n",
      "Processing 33 中文 Clozes...\n",
      "0h 01m 29s has elapsed for 1 Cloze notes of 33 (32 remaining).\n",
      "0h 01m 32s has elapsed for 2 Cloze notes of 33 (31 remaining).\n",
      "0h 01m 34s has elapsed for 3 Cloze notes of 33 (30 remaining).\n",
      "0h 01m 37s has elapsed for 4 Cloze notes of 33 (29 remaining).\n",
      "0h 01m 39s has elapsed for 5 Cloze notes of 33 (28 remaining).\n",
      "0h 01m 42s has elapsed for 6 Cloze notes of 33 (27 remaining).\n",
      "0h 01m 44s has elapsed for 7 Cloze notes of 33 (26 remaining).\n",
      "0h 01m 47s has elapsed for 8 Cloze notes of 33 (25 remaining).\n",
      "0h 01m 49s has elapsed for 9 Cloze notes of 33 (24 remaining).\n",
      "0h 01m 52s has elapsed for 10 Cloze notes of 33 (23 remaining).\n",
      "0h 01m 54s has elapsed for 11 Cloze notes of 33 (22 remaining).\n",
      "0h 01m 56s has elapsed for 12 Cloze notes of 33 (21 remaining).\n",
      "0h 01m 59s has elapsed for 13 Cloze notes of 33 (20 remaining).\n",
      "0h 02m 01s has elapsed for 14 Cloze notes of 33 (19 remaining).\n",
      "0h 02m 04s has elapsed for 15 Cloze notes of 33 (18 remaining).\n",
      "0h 02m 06s has elapsed for 16 Cloze notes of 33 (17 remaining).\n",
      "0h 02m 08s has elapsed for 17 Cloze notes of 33 (16 remaining).\n",
      "0h 02m 11s has elapsed for 18 Cloze notes of 33 (15 remaining).\n",
      "0h 02m 13s has elapsed for 19 Cloze notes of 33 (14 remaining).\n",
      "0h 02m 16s has elapsed for 20 Cloze notes of 33 (13 remaining).\n",
      "0h 02m 18s has elapsed for 21 Cloze notes of 33 (12 remaining).\n",
      "0h 02m 20s has elapsed for 22 Cloze notes of 33 (11 remaining).\n",
      "0h 02m 23s has elapsed for 23 Cloze notes of 33 (10 remaining).\n",
      "0h 02m 25s has elapsed for 24 Cloze notes of 33 (9 remaining).\n",
      "0h 02m 28s has elapsed for 25 Cloze notes of 33 (8 remaining).\n",
      "0h 02m 30s has elapsed for 26 Cloze notes of 33 (7 remaining).\n",
      "0h 02m 33s has elapsed for 27 Cloze notes of 33 (6 remaining).\n",
      "0h 02m 35s has elapsed for 28 Cloze notes of 33 (5 remaining).\n",
      "0h 02m 37s has elapsed for 29 Cloze notes of 33 (4 remaining).\n",
      "0h 02m 40s has elapsed for 30 Cloze notes of 33 (3 remaining).\n",
      "0h 02m 42s has elapsed for 31 Cloze notes of 33 (2 remaining).\n",
      "0h 02m 44s has elapsed for 32 Cloze notes of 33 (1 remaining).\n",
      "0h 02m 47s has elapsed for 33 Cloze notes of 33 (0 remaining).\n",
      "Updating 中文 Databases...\n",
      "ALL PROCESSES COMPLETED! ELAPSED TIME: 0h 02m 47s\n"
     ]
    }
   ],
   "source": [
    "# Define the current date and time\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "startTime1 = time.time()\n",
    "# Define the three functions we'll be using\n",
    "# Define Vocab Notes function\n",
    "def addVocab(language):\n",
    "    n = 0\n",
    "    # Handle the unique 日本語\n",
    "    if language == '日本語':\n",
    "    # Loop through all the rows and create a note for each one\n",
    "        for index, vocab in enumerate(vocabForeign):\n",
    "            try:\n",
    "                invoke('addNote', note = {\n",
    "                            \"deckName\": '{}'.format(language),\n",
    "                            \"modelName\": '(NEW) {}'.format(language),\n",
    "                            \"fields\": {\n",
    "                                \"(Word) Foreign\": vocab, \n",
    "                                \"(Word) English\": vocabEnglish[index],\n",
    "                                \"(Word) Foreign [漢字]\": kanji[index]\n",
    "                                }  \n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "            n += 1\n",
    "            print(\"{} has elapsed for {} Vocab words of {} ({} remaining).\".format(convert(time.time() - startTime1), n, words.shape[0], words.shape[0] - n))\n",
    "    # For 한국어 and 中文\n",
    "    else:\n",
    "        # Loop through all the rows and create a note for each one\n",
    "        for index, vocab in enumerate(vocabForeign):\n",
    "            try:\n",
    "                invoke('addNote', note = {\n",
    "                            \"deckName\": '{}'.format(language),\n",
    "                            \"modelName\": '(NEW) {}'.format(language),\n",
    "                            \"fields\": {\n",
    "                                \"(Word) Foreign\": vocab, \n",
    "                                \"(Word) English\": vocabEnglish[index]\n",
    "                                }  \n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "            n += 1\n",
    "            print(\"{} has elapsed for {} Vocab words of {} ({} remaining).\".format(convert(time.time() - startTime1), n, words.shape[0], words.shape[0] - n))\n",
    "# Define Cloze Notes function\n",
    "def addCloze(language):\n",
    "    n = 0\n",
    "    # Loop through all the rows and create a note for each one\n",
    "    for index, sentence in enumerate(cloze):\n",
    "        try:\n",
    "            invoke('addNote', note = {\n",
    "                        \"deckName\": '{}'.format(language),\n",
    "                        \"modelName\": '(NEW) {} Cloze'.format(language),\n",
    "                        \"fields\": {\n",
    "                            \"Text\": sentence, \n",
    "                            \"Tip\": tip[index],\n",
    "                            \"English\": english[index],\n",
    "                            \"full setence\": fullSentence[index]\n",
    "                            }  \n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "        n += 1\n",
    "        print(\"{} has elapsed for {} Cloze notes of {} ({} remaining).\".format(convert(time.time() - startTime1), n, dfm.shape[0], dfm.shape[0] - n))\n",
    "\n",
    "# Define the Sentence Scraping function\n",
    "\n",
    "def scrapeSentences(language):\n",
    "    words = vocab['Foreign']\n",
    "    n = 0\n",
    "    o = words.shape[0]\n",
    "    # Create master DataFrame\n",
    "    w = {}\n",
    "    concatList = []\n",
    "    # Run the scraper based on the language\n",
    "    if language == '日本語':\n",
    "        # Not done\n",
    "        # See CSV_Gen [ALL].ipynb\n",
    "        pass\n",
    "    elif language == '中文':\n",
    "        # Define URL for Chinese words\n",
    "        url_zh = 'https://dict.naver.com/linedict/zhendict/dict.html#/cnen/home'\n",
    "        # Define the driver for Selenium to use\n",
    "        driver = webdriver.Chrome('chromedriver.exe')\n",
    "        # Open the specfied URL\n",
    "        driver.get(url_zh)\n",
    "        # Find the radio group and select \"Examples\"\n",
    "        examples_radio = driver.find_element_by_xpath('//*[@id=\"ac_form\"]/fieldset/ul/li[2]/span').click()\n",
    "        # Find the Search bar\n",
    "        search = driver.find_element_by_xpath('//*[@id=\"ac_input\"]')\n",
    "        # Scrape sentences for each word present in the vocabulary list\n",
    "        for word in words:\n",
    "            # Type the word into the Search Box\n",
    "            search.send_keys(word)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(3)\n",
    "            # Find the sentences examples\n",
    "            examples = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]')\n",
    "            if examples is not None:\n",
    "                # Sleep to avoid errors\n",
    "                time.sleep(1)\n",
    "                # Define an object with all the example sentences\n",
    "                unfiltered_sentences = examples.text\n",
    "                # Clean the string of text that was just scraped\n",
    "                pass1 = unfiltered_sentences.replace(\"more\", \"\")\n",
    "                # Finalize the DataFrame\n",
    "                myList = pass1.split(\"\\n\")\n",
    "                myList = myList[::2]\n",
    "                # Drop the 拼音 be creating a dictionary\n",
    "                myDict = dict(zip(myList[::2], myList[1::2]))\n",
    "                foreign = myDict.keys()\n",
    "                english = myDict.values()\n",
    "                df = pd.DataFrame(columns = ['English', 'Foreign', 'Word', 'Hint', 'Cloze'])\n",
    "                df['English'] = english\n",
    "                df['Foreign'] = foreign\n",
    "                df['Word'] = word\n",
    "                wordsClean = words.tolist()\n",
    "                wordsClean = str(wordsClean).strip(\"[]\")\n",
    "                wordsClean = wordsClean.replace(\"'\", \"\")\n",
    "                df['Hint'] = wordsClean\n",
    "                df['Hint'] = df['Hint'].str.strip('[]')\n",
    "                df['Hint'] = df['Hint'].str.replace(\"'\", \"\")\n",
    "                # Create the Cloze sentences\n",
    "                df['Cloze'] = df['Foreign'].str.replace(word, '{{c1::' + word + '}}')\n",
    "                # Remove examples that don't contain a period\n",
    "                df['English'] = df['English'].map(str)\n",
    "                df1 = df[df['English'].str.contains('\\.', na = False)]\n",
    "                # Reduce the DataFrame to only 3 sentences\n",
    "                df2 = df1.iloc[:3]\n",
    "                # Create the final DataFrame for the word\n",
    "                w[word] = df2\n",
    "                # Add the DataFrame to the list of all word DataFrames\n",
    "                concatList.append(w[word])\n",
    "                # Sleep to avoid errors\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                pass\n",
    "            n += 1\n",
    "            o -= 1\n",
    "            print(\"Elapsed time: ({}) {} for {} words ({} remaining)\".format(word, convert(time.time() - startTime1), n, o))\n",
    "            # Exit the FOR loop\n",
    "        # Quit the browser\n",
    "        driver.quit()\n",
    "        # Concatenate each word's DataFrame to the final one with all the sentences\n",
    "        dfm = pd.concat(concatList, ignore_index = True)\n",
    "        # Fix formating issues\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('Name:.*', '')\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('\\n', ',')\n",
    "        #dfm['Cloze'] = dfm['Cloze'].str.replace('\\d', '')\n",
    "        # return the created DataFrame\n",
    "        return dfm\n",
    "    elif language == '한국어':\n",
    "        # Define URL for Korean words\n",
    "        url_ko = 'https://en.dict.naver.com/#/search?range=example&shouldSearchVlive=false&query=%ED%9B%84%ED%9A%8C&exampleLevel=exist:1&haveTrans=exist:1'\n",
    "        # Define the driver for Selenium to use\n",
    "        driver = webdriver.Chrome('chromedriver.exe')\n",
    "        # Open the specfied URL\n",
    "        driver.get(url_ko)\n",
    "        # Scrape sentences for each word present in the vocabulary list\n",
    "        for word in words:\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2)\n",
    "            # Find the Search bar\n",
    "            search = driver.find_element_by_xpath('//*[@id=\"ac_input\"]')\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Type the word into the Search Box\n",
    "            search.clear()\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            search.send_keys(word)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(3)\n",
    "            # Find the button titled \"Examples\" and click it\n",
    "            example_button = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[1]/div/div/a[4]')\n",
    "            example_button.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2)\n",
    "            # Find the \"Have Translations\" check box and check it\n",
    "            have_translations = driver.find_element_by_xpath('//*[@id=\"searchPage_example\"]/div[1]/div/span[5]/label').click()\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2.5)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2)\n",
    "            # Find the sentence examples\n",
    "            examples = driver.find_element_by_xpath('//*[@id=\"searchPage_example\"]/div[2]')\n",
    "            # Define an object with all the example sentences\n",
    "            unfiltered_sentences = examples.text\n",
    "            pass1 = unfiltered_sentences.replace(\"발음듣기\", \"\")\n",
    "            pass2 = pass1.replace(\"Oxford Advanced Learner's English-Korean Dictionary\", \"\")\n",
    "            pass3 = pass2.replace(\"절대어휘 5100\", \"\")\n",
    "            pass4 = pass3.replace(\"국제영어대학원대학교 신어사전\", \"\")\n",
    "            pass5 = pass4.replace(\"DARAKWON\", \"\")\n",
    "            pass6 = pass5.replace(\"Friendict Level English-Korean Dictionary\", \"\")\n",
    "            pass7 = pass6.replace(\"Web Collection\", \"\")\n",
    "            pass8 = pass7.replace(\"YBM All in All English-Korean Dictionary\", \"\")\n",
    "            pass9 = pass8.replace(\"Neungyule\", \"\")\n",
    "            pass10 = pass9.replace(\"User translation\", \"\")\n",
    "            pass11 = pass10.replace(\"Hansard\", \"\")\n",
    "            pass12 = pass11.replace(\"영어영작문대사전\", \"\")\n",
    "            pass13 = pass12.replace(\"English Hidden Card\", \"\")\n",
    "            pass14 = pass13.replace(\"TIMES CORE\", \"\")\n",
    "            pass15 = pass14.replace(\"Dong-a's Prime English-Korean Dictionary\", \"\")\n",
    "            # Create a Dictionary with which to create the final DataFrame\n",
    "            myList = pass15.split(\"\\n\\n\")\n",
    "            # Remove the parentheitcal words from the list\n",
    "            myList2 = []\n",
    "            for i in myList:\n",
    "                i = re.sub(r'\\([^()]*\\)', '', i)\n",
    "                i = i.lstrip()\n",
    "                i = i.rstrip(\"\\n\")\n",
    "                myList2.append(i)\n",
    "            # Make dictionary from the new list\n",
    "            myDict = dict(zip(myList2[::2], myList2[1::2]))\n",
    "            english = myDict.keys()\n",
    "            foreign = myDict.values()\n",
    "            # Create a DataFrame from the scraped string\n",
    "            df = pd.DataFrame(columns = ['English', 'Foreign', 'Word', 'Hint', 'Cloze'])\n",
    "            df['English'] = english\n",
    "            df['Foreign'] = foreign\n",
    "            df['Word'] = word\n",
    "            wordsClean = words.tolist()\n",
    "            wordsClean = str(wordsClean).strip(\"[]\")\n",
    "            wordsClean = wordsClean.replace(\"'\", \"\")\n",
    "            df['Hint'] = wordsClean\n",
    "            df['Hint'] = df['Hint'].str.replace(\"'\", \"\")\n",
    "            # Handle conjugations\n",
    "            if \"하다\" in word:\n",
    "                wordConj = word[:-2]\n",
    "            elif \"다\" in word:\n",
    "                wordConj = word[:-1]\n",
    "            else:\n",
    "                wordConj = word\n",
    "            # Create the Cloze sentences    \n",
    "            df['Cloze'] = df['Foreign'].str.replace(wordConj, '{{c1::' + wordConj + '}}')\n",
    "            # Remove examples that don't contain a period\n",
    "            df['English'] = df['English'].map(str)\n",
    "            # Reduce the DataFrame to only 3 sentences\n",
    "            df2 = df.iloc[:3]\n",
    "            # Create the final DataFrame for the word\n",
    "            w[word] = df2\n",
    "            # Add the DataFrame to the list of all word DataFrames\n",
    "            concatList.append(w[word])\n",
    "            n += 1\n",
    "            o -= 1\n",
    "            print(\"Elapsed time: ({}) {} for {} words ({} remaining)\".format(word, convert(time.time() - startTime1), n, o))\n",
    "            # Exit the FOR loop\n",
    "        # Quit the browser\n",
    "        driver.quit()\n",
    "        # Concatenate each word's DataFrame to the final one with all the sentences\n",
    "        dfm = pd.concat(concatList, ignore_index = True)\n",
    "        # Remove the pesky fuckers that got through the cleaning passes filters\n",
    "        dfm['Foreign'] = dfm['Foreign'].str.replace('\\n.*', '')\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('\\n.*', '')\n",
    "        # return the created DataFrame\n",
    "        return dfm\n",
    "        # Exit the IF-ELIF statement\n",
    "    elif language == '日本語':\n",
    "        pass\n",
    "\n",
    "# DEFINE FUNCTIONS\n",
    "\n",
    "def convert(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%dh %02dm %02ds\" % (hour, minutes, seconds)\n",
    "\n",
    "def convertTTS(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%d hours %02d minutes %02d seconds\" % (hour, minutes, seconds)\n",
    "\n",
    "# Define the Anki-Connect functions\n",
    "def request(action, **params):\n",
    "    return {'action': action, 'params': params, 'version': 6}\n",
    "def invoke(action, **params):\n",
    "    requestJson = json.dumps(request(action, **params)).encode('utf-8')\n",
    "    response = json.load(urllib.request.urlopen(urllib.request.Request('http://localhost:8765', requestJson)))\n",
    "    if len(response) != 2:\n",
    "        raise Exception('response has an unexpected number of fields')\n",
    "    if 'error' not in response:\n",
    "        raise Exception('response is missing required error field')\n",
    "    if 'result' not in response:\n",
    "        raise Exception('response is missing required result field')\n",
    "    if response['error'] is not None:\n",
    "        raise Exception(response['error'])\n",
    "    return response['result']\n",
    "\n",
    "# Define the Database Updating function\n",
    "def updateDatabase(language):\n",
    "    # Load the current Database into DataFrames\n",
    "    wordsDF = pd.read_excel(r'Databases\\{}_words.xlsx'.format(language), sheet_name = 'words', header = 0)\n",
    "    sentencesDF =  pd.read_excel(r'Databases\\{}_sentences.xlsx'.format(language), sheet_name = 'sentences', header = 0)\n",
    "    # Get today's date and store it as a string\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    if language != '日本語':  \n",
    "        # Format the DataFrame to be appended to our local Database\n",
    "        dfmAppend = dfm.drop(axis = 1, columns = ['Hint', 'Cloze'])\n",
    "        # Name the columns in accordance with Flashcards.xlsx\n",
    "        dfmAppend.columns = ['ENGLISH', 'FOREIGN', 'WORD']\n",
    "        # Set the date of each row to today's date\n",
    "        dfmAppend['DATE'] = date\n",
    "    elif language == '日本語':\n",
    "        pass\n",
    "    # Create the Vocabulary DataFrame that will be appended \n",
    "    if language != '日本語':\n",
    "        vocabAppend = vocab\n",
    "        vocabAppend.columns = ['FOREIGN', 'ENGLISH']\n",
    "    elif language == '日本語':\n",
    "        vocabAppend = vocab\n",
    "        vocabAppend.columns = ['FOREIGN', 'ENGLISH', 'KANJI']\n",
    "    vocabAppend['DATE'] = date\n",
    "\n",
    "    # Loop through the vocabAppend DF to find duplicates\n",
    "    for index, row in vocabAppend.iterrows():\n",
    "        word = vocabAppend['FOREIGN'][index]\n",
    "        if wordsDF['FOREIGN'].str.contains(word).sum() > 0:\n",
    "            vocabAppend.drop(axis = 0, index = index, inplace = True)\n",
    "        \n",
    "    # Loop through the dfmAppend DF to find duplicates\n",
    "    for index, row in dfmAppend.iterrows():\n",
    "        word = dfmAppend['WORD'][index]\n",
    "        if wordsDF['FOREIGN'].str.contains(word).sum() > 0:\n",
    "            dfmAppend.drop(axis = 0, index = index, inplace = True)\n",
    "    # Append the new rows to the DataFrames\n",
    "    wordsDF = wordsDF.append(vocabAppend)\n",
    "\n",
    "    # Create new XLSX files including the appended rows\n",
    "    wordsDF.to_excel('Databases\\{}_words.xlsx'.format(language), sheet_name = 'words', index = False)\n",
    "\n",
    "    if language != '日本語':\n",
    "        sentencesDF = sentencesDF.append(dfmAppend)\n",
    "        sentencesDF.to_excel('Databases\\{}_sentences.xlsx'.format(language), sheet_name = 'sentences', index = False)\n",
    "    elif language == '日本語':\n",
    "        pass\n",
    "\n",
    "# THE REAL WORK BEGINS\n",
    "\n",
    "# Read CSVs into DataFrames\n",
    "vocab = pd.read_excel(r'Flashcards.xlsx', sheet_name = 'vocab', header = 0)\n",
    "vocab.columns = ['Foreign', 'English', 'Kanji', 'Language']\n",
    "vocab['English'] = vocab['English'].replace('s/\\([^)]*\\)//', '')\n",
    "# Retrieve the language from the vocab DataFrame then drop its column\n",
    "language = vocab['Language'].loc[0:0].to_string(index = False, header = False)\n",
    "vocab.drop(['Language'], inplace = True, axis = 1)\n",
    "# Strip the leading space from the lang variable (Thank you shit Excel dropdown menu! /s)\n",
    "language = language.lstrip()\n",
    "# Drop nan values in the Vocab DataFrame\n",
    "if language == '日本語':\n",
    "    vocab.dropna(axis = 0, inplace = True)\n",
    "elif language == '中文' or '한국어': \n",
    "    vocab.drop(['Kanji'], inplace = True, axis = 1)\n",
    "    vocab.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# Define the returned DataFrame from scrapeSentences()\n",
    "words = vocab['Foreign']\n",
    "print(\"Processing {} {} words for scraping...\".format(words.shape[0], language))\n",
    "dfm = scrapeSentences(language)\n",
    "# Define the necessary variables\n",
    "vocabForeign = vocab['Foreign'].to_list()\n",
    "vocabEnglish = vocab['English'].to_list()\n",
    "# Define Kanji if it exists\n",
    "if 'Kanji' in vocab.columns:\n",
    "    kanji = vocab['Kanji'].to_list()\n",
    "else:\n",
    "    pass\n",
    "# Add the Notes\n",
    "print(\"Processing {} {} Vocabulary...\".format(words.shape[0], language))\n",
    "addVocab(language)\n",
    "# Define the necessary variables\n",
    "if language != '日本語':\n",
    "    cloze = dfm['Cloze'].to_list()\n",
    "    tip = dfm['Hint'].to_list()\n",
    "    english = dfm['English'].to_list()\n",
    "    fullSentence = dfm['Foreign'].to_list()\n",
    "    print(\"Processing {} {} Clozes...\".format(dfm.shape[0], language))\n",
    "    addCloze(language)\n",
    "print(\"Updating {} Databases...\".format(language))\n",
    "updateDatabase(language)\n",
    "print(\"ALL PROCESSES COMPLETED! ELAPSED TIME: {}\".format(convert(time.time() - startTime1)))\n",
    "tts = gTTS(text = \"All processes have completed in {}.\".format(convertTTS(time.time() - startTime1)), lang = \"en\")\n",
    "tts.save(\"tts.mp3\")\n",
    "playsound(\"tts.mp3\")\n",
    "os.remove(\"tts.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateDatabase(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.read_excel(r'Databases\\{}_sentences.xlsx'.format(language), sheet_name = 'sentences', header = 0)\n",
    "start = 236\n",
    "for index, row in sent.iterrows():\n",
    "    sent['CLOZE'][index] = sent['FOREIGN'][index].replace(sent['WORD'][index], '{{c1::' + sent['WORD'][index] + '}}')\n",
    "sentences = sent['CLOZE']\n",
    "english = sent['ENGLISH'][start:].to_list()\n",
    "tip = sent['WORD'][start:].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addCloze(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}