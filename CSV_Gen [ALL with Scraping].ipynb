{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Scraping Definitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有道词典 definitions \n",
    "defs = driver.find_element_by_xpath('//*[@id=\"phrsListTab\"]/div[2]/ul/p').text\n",
    "# LINE Dict. definitions\n",
    "defs = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[3]/ol').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import re\n",
    "from io import StringIO\n",
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import playsound\n",
    "from numpy import random\n",
    "from datetime import datetime\n",
    "from langdetect import detect\n",
    "import winsound\n",
    "from playsound import playsound\n",
    "from gtts import gTTS\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "source": [
    "## How Do I Study Today?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Listening Today\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 21b0a0fbf47b0f12a82b38dd710322068cf94680
   "source": [
    "file = open('what_do.txt', mode = 'r')\n",
    "what_do = file.read()\n",
    "file.close()\n",
    "if what_do == 'listening':\n",
    "    print(\"Listening Today\")\n",
    "    f = open('what_do.txt', 'w+')\n",
    "    f.write('reading')\n",
    "    f.close()\n",
    "elif what_do == 'reading':\n",
    "    print(\"Reading Today\")\n",
    "    f = open('what_do.txt', 'w+')\n",
    "    f.write('listening')\n",
    "    f.close()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## What Words Do I Use to Write Sentences?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------中文--------\n",
      "Series([], Name: FOREIGN, dtype: object)\n",
      "\n",
      "--------한국어--------\n",
      "Series([], Name: FOREIGN, dtype: object)\n",
      "\n",
      "--------日本語--------\n",
      "Series([], Name: FOREIGN, dtype: object)\n",
      "\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 21b0a0fbf47b0f12a82b38dd710322068cf94680
   "source": [
    "languages = ['中文', '한국어', '日本語']\n",
    "sentenceDate = datetime.now().date()\n",
    "for i in languages:\n",
    "    # Read the Excel files into DataFrames\n",
    "    wordsDF = pd.read_excel(r'Databases\\{}_words.xlsx'.format(i), sheet_name = 'words', header = 0)\n",
    "    sentencesDF =  pd.read_excel(r'Databases\\{}_sentences.xlsx'.format(i), sheet_name = 'sentences', header = 0)\n",
    "    # Read the Chinese Databases into DataFrames\n",
    "    words = pd.read_excel(r'Databases\\{}_words.xlsx'.format(i), sheet_name = 'words', header = 0)\n",
    "    # Define the date that we want to retreive\n",
    "    desired_date = (sentenceDate - timedelta(days = 14)).strftime(\"%Y-%m-%d\")\n",
    "    # Retrieve the words from the specified date\n",
    "    write = wordsDF['FOREIGN'].where(words['DATE'] == desired_date)\n",
    "    # Drop NAN rows\n",
    "    write = write.dropna().reset_index(drop = True)\n",
    "    # Display the words with which to write sentences\n",
    "    print(\"--------{}--------\\n{}\\n\".format(i, write))"
   ]
  },
  {
   "source": [
    "## The Importer + Scraper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing 7 한국어 words for scraping...\n",
      "Elapsed time: (분야) 0h 00m 20s for 1 words (6 remaining)\n",
      "Elapsed time: (진출하다) 0h 00m 36s for 2 words (5 remaining)\n",
      "Elapsed time: (가난하다) 0h 00m 52s for 3 words (4 remaining)\n",
      "Elapsed time: (신혼여행) 0h 01m 07s for 4 words (3 remaining)\n",
      "Elapsed time: (경우) 0h 01m 23s for 5 words (2 remaining)\n",
      "Elapsed time: (장난) 0h 01m 39s for 6 words (1 remaining)\n",
      "Elapsed time: (장난꾸러기) 0h 01m 55s for 7 words (0 remaining)\n",
      "Processing 7 한국어 Vocabulary...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "cannot create note because it is a duplicate",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c36ba236846d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;31m# Add the Notes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Processing {} {} Vocabulary...\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m \u001b[0maddVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;31m# Define the necessary variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'日本語'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c36ba236846d>\u001b[0m in \u001b[0;36maddVocab\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Loop through all the rows and create a note for each one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabForeign\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             invoke('addNote', note = {\n\u001b[0m\u001b[0;32m     28\u001b[0m                         \u001b[1;34m\"deckName\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[1;34m\"modelName\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'(NEW) {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c36ba236846d>\u001b[0m in \u001b[0;36minvoke\u001b[1;34m(action, **params)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'response is missing required result field'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: cannot create note because it is a duplicate"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 21b0a0fbf47b0f12a82b38dd710322068cf94680
   "source": [
    "# Define the current date and time\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "startTime1 = time.time()\n",
    "# Define the three functions we'll be using\n",
    "# Define Vocab Notes function\n",
    "def addVocab(language):\n",
    "    n = 0\n",
    "    # Handle the unique 日本語\n",
    "    if language == '日本語':\n",
    "    # Loop through all the rows and create a note for each one\n",
    "        for index, vocab in enumerate(vocabForeign):\n",
    "            invoke('addNote', note = {\n",
    "                        \"deckName\": '{}'.format(language),\n",
    "                        \"modelName\": '(NEW) {}'.format(language),\n",
    "                        \"fields\": {\n",
    "                            \"(Word) Foreign\": vocab, \n",
    "                            \"(Word) English\": vocabEnglish[index],\n",
    "                            \"(Word) Foreign [漢字]\": kanji[index]\n",
    "                            }  \n",
    "                })\n",
    "            n += 1\n",
    "            print(\"{} has elapsed for {} Vocab words of {} ({} remaining).\".format(convert(time.time() - startTime1), n, words.shape[0], words.shape[0] - n))\n",
    "    # For 한국어 and 中文\n",
    "    else:\n",
    "        # Loop through all the rows and create a note for each one\n",
    "        for index, vocab in enumerate(vocabForeign):\n",
    "            invoke('addNote', note = {\n",
    "                        \"deckName\": '{}'.format(language),\n",
    "                        \"modelName\": '(NEW) {}'.format(language),\n",
    "                        \"fields\": {\n",
    "                            \"(Word) Foreign\": vocab, \n",
    "                            \"(Word) English\": vocabEnglish[index]\n",
    "                            }  \n",
    "                })\n",
    "            n += 1\n",
    "            print(\"{} has elapsed for {} Vocab words of {} ({} remaining).\".format(convert(time.time() - startTime1), n, words.shape[0], words.shape[0] - n))\n",
    "# Define Cloze Notes function\n",
    "def addCloze(language):\n",
    "    n = 0\n",
    "    # Loop through all the rows and create a note for each one\n",
    "    for index, sentence in enumerate(cloze):\n",
    "        invoke('addNote', note = {\n",
    "                    \"deckName\": '{}'.format(language),\n",
    "                    \"modelName\": '(NEW) {} Cloze'.format(language),\n",
    "                    \"fields\": {\n",
    "                        \"Text\": sentence, \n",
    "                        \"Tip\": tip[index],\n",
    "                        \"English\": english[index],\n",
    "                        \"full setence\": fullSentence[index]\n",
    "                        }  \n",
    "            })\n",
    "        n += 1\n",
    "        print(\"{} has elapsed for {} Cloze notes of {} ({} remaining).\".format(convert(time.time() - startTime1), n, dfm.shape[0], dfm.shape[0] - n))\n",
    "\n",
    "# Define the Sentence Scraping function\n",
    "\n",
    "def scrapeSentences(language):\n",
    "    words = vocab['Foreign']\n",
    "    n = 0\n",
    "    o = words.shape[0]\n",
    "    # Create master DataFrame\n",
    "    w = {}\n",
    "    concatList = []\n",
    "    # Run the scraper based on the language\n",
    "    if language == '日本語':\n",
    "        # Not done\n",
    "        # See CSV_Gen [ALL].ipynb\n",
    "        pass\n",
    "    elif language == '中文':\n",
    "        # Define URL for Chinese words\n",
    "        url_zh = 'https://dict.naver.com/linedict/zhendict/dict.html#/cnen/home'\n",
    "        # Define the driver for Selenium to use\n",
    "        driver = webdriver.Chrome('chromedriver.exe')\n",
    "        # Open the specfied URL\n",
    "        driver.get(url_zh)\n",
    "        # Find the radio group and select \"Examples\"\n",
    "        examples_radio = driver.find_element_by_xpath('//*[@id=\"ac_form\"]/fieldset/ul/li[2]/span').click()\n",
    "        # Find the Search bar\n",
    "        search = driver.find_element_by_xpath('//*[@id=\"ac_input\"]')\n",
    "        # Scrape sentences for each word present in the vocabulary list\n",
    "        for word in words:\n",
    "            # Type the word into the Search Box\n",
    "            search.send_keys(word)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(3)\n",
    "            # Find the sentences examples\n",
    "            examples = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]')\n",
    "            if examples is not None:\n",
    "                # Sleep to avoid errors\n",
    "                time.sleep(1)\n",
    "                # Define an object with all the example sentences\n",
    "                unfiltered_sentences = examples.text\n",
    "                # Clean the string of text that was just scraped\n",
    "                pass1 = unfiltered_sentences.replace(\"more\", \"\")\n",
    "                # Finalize the DataFrame\n",
    "                myList = pass1.split(\"\\n\")\n",
    "                myList = myList[::2]\n",
    "                # Drop the 拼音 be creating a dictionary\n",
    "                myDict = dict(zip(myList[::2], myList[1::2]))\n",
    "                foreign = myDict.keys()\n",
    "                english = myDict.values()\n",
    "                df = pd.DataFrame(columns = ['English', 'Foreign', 'Word', 'Hint', 'Cloze'])\n",
    "                df['English'] = english\n",
    "                df['Foreign'] = foreign\n",
    "                df['Word'] = word\n",
    "                wordsClean = words.tolist()\n",
    "                wordsClean = str(wordsClean).strip(\"[]\")\n",
    "                wordsClean = wordsClean.replace(\"'\", \"\")\n",
    "                df['Hint'] = wordsClean\n",
    "                df['Hint'] = df['Hint'].str.strip('[]')\n",
    "                df['Hint'] = df['Hint'].str.replace(\"'\", \"\")\n",
    "                # Create the Cloze sentences\n",
    "                df['Cloze'] = df['Foreign'].str.replace(word, '{{c1::' + word + '}}')\n",
    "                # Remove examples that don't contain a period\n",
    "                df['English'] = df['English'].map(str)\n",
    "                df1 = df[df['English'].str.contains('\\.', na = False)]\n",
    "                # Reduce the DataFrame to only 3 sentences\n",
    "                df2 = df1.iloc[:3]\n",
    "                # Create the final DataFrame for the word\n",
    "                w[word] = df2\n",
    "                # Add the DataFrame to the list of all word DataFrames\n",
    "                concatList.append(w[word])\n",
    "                # Sleep to avoid errors\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                pass\n",
    "            n += 1\n",
    "            o -= 1\n",
    "            print(\"Elapsed time: ({}) {} for {} words ({} remaining)\".format(word, convert(time.time() - startTime1), n, o))\n",
    "            # Exit the FOR loop\n",
    "        # Quit the browser\n",
    "        driver.quit()\n",
    "        # Concatenate each word's DataFrame to the final one with all the sentences\n",
    "        dfm = pd.concat(concatList, ignore_index = True)\n",
    "        # Fix formating issues\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('Name:.*', '')\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('\\n', ',')\n",
    "        #dfm['Cloze'] = dfm['Cloze'].str.replace('\\d', '')\n",
    "        # return the created DataFrame\n",
    "        return dfm\n",
    "    elif language == '한국어':\n",
    "        # Define URL for Korean words\n",
    "        url_ko = 'https://en.dict.naver.com/#/search?range=example&shouldSearchVlive=false&query=%ED%9B%84%ED%9A%8C&exampleLevel=exist:1&haveTrans=exist:1'\n",
    "        # Define the driver for Selenium to use\n",
    "        driver = webdriver.Chrome('chromedriver.exe')\n",
    "        # Open the specfied URL\n",
    "        driver.get(url_ko)\n",
    "        # Scrape sentences for each word present in the vocabulary list\n",
    "        for word in words:\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2)\n",
    "            # Find the Search bar\n",
    "            search = driver.find_element_by_xpath('//*[@id=\"ac_input\"]')\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Type the word into the Search Box\n",
    "            search.clear()\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            search.send_keys(word)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(3)\n",
    "            # Find the button titled \"Examples\" and click it\n",
    "            example_button = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[1]/div/div/a[4]')\n",
    "            example_button.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2)\n",
    "            # Find the \"Have Translations\" check box and check it\n",
    "            have_translations = driver.find_element_by_xpath('//*[@id=\"searchPage_example\"]/div[1]/div/span[5]/label').click()\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2.5)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2)\n",
    "            # Find the sentence examples\n",
    "            examples = driver.find_element_by_xpath('//*[@id=\"searchPage_example\"]/div[2]')\n",
    "            # Define an object with all the example sentences\n",
    "            unfiltered_sentences = examples.text\n",
    "            pass1 = unfiltered_sentences.replace(\"발음듣기\", \"\")\n",
    "            pass2 = pass1.replace(\"Oxford Advanced Learner's English-Korean Dictionary\", \"\")\n",
    "            pass3 = pass2.replace(\"절대어휘 5100\", \"\")\n",
    "            pass4 = pass3.replace(\"국제영어대학원대학교 신어사전\", \"\")\n",
    "            pass5 = pass4.replace(\"DARAKWON\", \"\")\n",
    "            pass6 = pass5.replace(\"Friendict Level English-Korean Dictionary\", \"\")\n",
    "            pass7 = pass6.replace(\"Web Collection\", \"\")\n",
    "            pass8 = pass7.replace(\"YBM All in All English-Korean Dictionary\", \"\")\n",
    "            pass9 = pass8.replace(\"Neungyule\", \"\")\n",
    "            pass10 = pass9.replace(\"User translation\", \"\")\n",
    "            pass11 = pass10.replace(\"Hansard\", \"\")\n",
    "            pass12 = pass11.replace(\"영어영작문대사전\", \"\")\n",
    "            pass13 = pass12.replace(\"English Hidden Card\", \"\")\n",
    "            pass14 = pass13.replace(\"TIMES CORE\", \"\")\n",
    "            pass15 = pass14.replace(\"Dong-a's Prime English-Korean Dictionary\", \"\")\n",
    "            # Create a Dictionary with which to create the final DataFrame\n",
    "            myList = pass15.split(\"\\n\\n\")\n",
    "            # Remove the parentheitcal words from the list\n",
    "            myList2 = []\n",
    "            for i in myList:\n",
    "                i = re.sub(r'\\([^()]*\\)', '', i)\n",
    "                i = i.lstrip()\n",
    "                i = i.rstrip(\"\\n\")\n",
    "                myList2.append(i)\n",
    "            # Make dictionary from the new list\n",
    "            myDict = dict(zip(myList2[::2], myList2[1::2]))\n",
    "            english = myDict.keys()\n",
    "            foreign = myDict.values()\n",
    "            # Create a DataFrame from the scraped string\n",
    "            df = pd.DataFrame(columns = ['English', 'Foreign', 'Word', 'Hint', 'Cloze'])\n",
    "            df['English'] = english\n",
    "            df['Foreign'] = foreign\n",
    "            df['Word'] = word\n",
    "            wordsClean = words.tolist()\n",
    "            wordsClean = str(wordsClean).strip(\"[]\")\n",
    "            wordsClean = wordsClean.replace(\"'\", \"\")\n",
    "            df['Hint'] = wordsClean\n",
    "            df['Hint'] = df['Hint'].str.replace(\"'\", \"\")\n",
    "            # Handle conjugations\n",
    "            if \"하다\" in word:\n",
    "                wordConj = word[:-2]\n",
    "            elif \"다\" in word:\n",
    "                wordConj = word[:-1]\n",
    "            else:\n",
    "                wordConj = word\n",
    "            # Create the Cloze sentences    \n",
    "            df['Cloze'] = df['Foreign'].str.replace(wordConj, '{{c1::' + wordConj + '}}')\n",
    "            # Remove examples that don't contain a period\n",
    "            df['English'] = df['English'].map(str)\n",
    "            # Reduce the DataFrame to only 3 sentences\n",
    "            df2 = df.iloc[:3]\n",
    "            # Create the final DataFrame for the word\n",
    "            w[word] = df2\n",
    "            # Add the DataFrame to the list of all word DataFrames\n",
    "            concatList.append(w[word])\n",
    "            n += 1\n",
    "            o -= 1\n",
    "            print(\"Elapsed time: ({}) {} for {} words ({} remaining)\".format(word, convert(time.time() - startTime1), n, o))\n",
    "            # Exit the FOR loop\n",
    "        # Quit the browser\n",
    "        driver.quit()\n",
    "        # Concatenate each word's DataFrame to the final one with all the sentences\n",
    "        dfm = pd.concat(concatList, ignore_index = True)\n",
    "        # Remove the pesky fuckers that got through the cleaning passes filters\n",
    "        dfm['Foreign'] = dfm['Foreign'].str.replace('\\n.*', '')\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('\\n.*', '')\n",
    "        # return the created DataFrame\n",
    "        return dfm\n",
    "        # Exit the IF-ELIF statement\n",
    "    elif language == '日本語':\n",
    "        pass\n",
    "\n",
    "# DEFINE FUNCTIONS\n",
    "\n",
    "def convert(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%dh %02dm %02ds\" % (hour, minutes, seconds)\n",
    "\n",
    "def convertTTS(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%d hours %02d minutes %02d seconds\" % (hour, minutes, seconds)\n",
    "\n",
    "# Define the Anki-Connect functions\n",
    "def request(action, **params):\n",
    "    return {'action': action, 'params': params, 'version': 6}\n",
    "def invoke(action, **params):\n",
    "    requestJson = json.dumps(request(action, **params)).encode('utf-8')\n",
    "    response = json.load(urllib.request.urlopen(urllib.request.Request('http://localhost:8765', requestJson)))\n",
    "    if len(response) != 2:\n",
    "        raise Exception('response has an unexpected number of fields')\n",
    "    if 'error' not in response:\n",
    "        raise Exception('response is missing required error field')\n",
    "    if 'result' not in response:\n",
    "        raise Exception('response is missing required result field')\n",
    "    if response['error'] is not None:\n",
    "        raise Exception(response['error'])\n",
    "    return response['result']\n",
    "\n",
    "# Define the Database Updating function\n",
    "def updateDatabase(language):\n",
    "    # Load the current Database into DataFrames\n",
    "    wordsDF = pd.read_excel(r'Databases\\{}_words.xlsx'.format(language), sheet_name = 'words', header = 0)\n",
    "    sentencesDF =  pd.read_excel(r'Databases\\{}_sentences.xlsx'.format(language), sheet_name = 'sentences', header = 0)\n",
    "    # Get today's date and store it as a string\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    if language != '日本語':  \n",
    "        # Format the DataFrame to be appended to our local Database\n",
    "        dfmAppend = dfm.drop(axis = 1, columns = ['Hint', 'Cloze'])\n",
    "        # Name the columns in accordance with Flashcards.xlsx\n",
    "        dfmAppend.columns = ['ENGLISH', 'FOREIGN', 'WORD']\n",
    "        # Set the date of each row to today's date\n",
    "        dfmAppend['DATE'] = date\n",
    "    else:\n",
    "        pass\n",
    "    # Create the Vocabulary DataFrame that will be updated \n",
    "    if language != '日本語':\n",
    "        vocabAppend = vocab\n",
    "        vocabAppend.columns = ['FOREIGN', 'ENGLISH']\n",
    "    else:\n",
    "        vocabAppend = vocab\n",
    "        vocabAppend.columns = ['FOREIGN', 'ENGLISH', 'KANJI']\n",
    "    vocabAppend['DATE'] = date\n",
    "\n",
    "    # Append the new rows to the DataFrames\n",
    "    wordsDF = wordsDF.append(vocabAppend)\n",
    "    \n",
    "    # Create new XLSX files with including the appended rows\n",
    "    wordsDF.to_excel('Databases\\{}_words.xlsx'.format(language), sheet_name = 'words', index = False)\n",
    "\n",
    "    if language != '日本語':\n",
    "        sentencesDF = sentencesDF.append(dfmAppend)\n",
    "        sentencesDF.to_excel('Databases\\{}_sentences.xlsx'.format(language), sheet_name = 'sentences', index = False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# THE REAL WORK BEGINS\n",
    "\n",
    "# Read CSVs into DataFrames\n",
    "vocab = pd.read_excel(r'Flashcards.xlsx', sheet_name = 'vocab', header = 0)\n",
    "vocab.columns = ['Foreign', 'English', 'Kanji', 'Language']\n",
    "vocab['English'] = vocab['English'].replace('s/\\([^)]*\\)//', '')\n",
    "# Retrieve the language from the vocab DataFrame then drop its column\n",
    "language = vocab['Language'].loc[0:0].to_string(index = False, header = False)\n",
    "vocab.drop(['Language'], inplace = True, axis = 1)\n",
    "# Strip the leading space from the lang variable (Thank you shit Excel dropdown menu! /s)\n",
    "language = language.lstrip()\n",
    "# Drop nan values in the Vocab DataFrame\n",
    "if language == '日本語':\n",
    "    vocab.dropna(axis = 0, inplace = True)\n",
    "elif language == '中文' or '한국어': \n",
    "    vocab.drop(['Kanji'], inplace = True, axis = 1)\n",
    "    vocab.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# Define the returned DataFrame from scrapeSentences()\n",
    "words = vocab['Foreign']\n",
    "print(\"Processing {} {} words for scraping...\".format(words.shape[0], language))\n",
    "dfm = scrapeSentences(language)\n",
    "# Define the necessary variables\n",
    "vocabForeign = vocab['Foreign'].to_list()\n",
    "vocabEnglish = vocab['English'].to_list()\n",
    "# Define Kanji if it exists\n",
    "if 'Kanji' in vocab.columns:\n",
    "    kanji = vocab['Kanji'].to_list()\n",
    "else:\n",
    "    pass\n",
    "# Add the Notes\n",
    "print(\"Processing {} {} Vocabulary...\".format(words.shape[0], language))\n",
    "addVocab(language)\n",
    "# Define the necessary variables\n",
    "if language != '日本語':\n",
    "    cloze = dfm['Cloze'].to_list()\n",
    "    tip = dfm['Hint'].to_list()\n",
    "    english = dfm['English'].to_list()\n",
    "    fullSentence = dfm['Foreign'].to_list()\n",
    "    print(\"Processing {} {} Clozes...\".format(dfm.shape[0], language))\n",
    "    addCloze(language)\n",
    "print(\"Updating {} Databases...\".format(language))\n",
    "updateDatabase(language)\n",
    "print(\"ALL PROCESSES COMPLETED! ELAPSED TIME: {}\".format(convert(time.time() - startTime1)))\n",
    "tts = gTTS(text = \"All processes have completed in {}.\".format(convertTTS(time.time() - startTime1)), lang = \"en\")\n",
    "tts.save(\"tts.mp3\")\n",
    "playsound(\"tts.mp3\")\n",
    "os.remove(\"tts.mp3\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> 21b0a0fbf47b0f12a82b38dd710322068cf94680
   "metadata": {},
   "outputs": [],
   "source": [
    "updateDatabase(language)"
   ]
  },
  {
   "source": [
    "## Add Clozes that were Duplicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = dfm[32:]\n",
    "sent = pd.read_excel(r'Databases\\{}_sentences.xlsx'.format(language), sheet_name = 'sentences', header = 0)\n",
    "sent = sent[240:243]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent['CLOZE'] = sent['FOREIGN']\n",
    "for index, row in sent.iterrows():\n",
    "    sent['CLOZE'][index] = sent['CLOZE'][index].replace(sent['WORD'][index], '{{c1::' + sent['WORD'][index] + '}}')\n",
    "sent.rename(columns = {'CLOZE': 'Cloze', 'ENGLISH': 'English', 'WORD': 'Word', 'FOREIGN': 'Foreign'}, inplace = True)\n",
    "sent = sent.drop(columns = ['DATE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             English  \\\n",
       "0  This kind of lever scale is very exact and is ...   \n",
       "1  Accurate estimates of the marginal costs of pr...   \n",
       "2  To obtain accurate results, you need to employ...   \n",
       "3                            All the chairs matched.   \n",
       "4               The turkey was trimmed with parsley.   \n",
       "5  A wide-brimmed hat will be  becoming with that...   \n",
       "6                          Reading makes a full man.   \n",
       "7             The legal system needs to be improved.   \n",
       "8                        Drainage here is imperfect.   \n",
       "\n",
       "                    Foreign Word  \\\n",
       "0        这杆秤非常精准，买菜的人都用它。     精准   \n",
       "1     精准的边际生产成本的估计是很难达到的。     精准   \n",
       "2  你需要使用精准的统计工具才能得到正确的结果。     精准   \n",
       "3              所有的椅子都很搭配。     搭配   \n",
       "4           这盘火鸡用欧芹来搭配点缀。     搭配   \n",
       "5            宽沿帽和她的毛衣更搭配。     搭配   \n",
       "6                 读书使人完善。     完善   \n",
       "7                  法律体系需要完善   完善   \n",
       "8               这里的排水设施不完善。   完善   \n",
       "\n",
       "                                                Hint  \\\n",
       "0  骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...   \n",
       "1  骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...   \n",
       "2  骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...   \n",
       "3  骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...   \n",
       "4  骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...   \n",
       "5  骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8                                                NaN   \n",
       "\n",
       "                              Cloze  \n",
       "0        这杆秤非常{{c1::精准}}，买菜的人都用它。    \n",
       "1     {{c1::精准}}的边际生产成本的估计是很难达到的。    \n",
       "2  你需要使用{{c1::精准}}的统计工具才能得到正确的结果。    \n",
       "3              所有的椅子都很{{c1::搭配}}。    \n",
       "4           这盘火鸡用欧芹来{{c1::搭配}}点缀。    \n",
       "5            宽沿帽和她的毛衣更{{c1::搭配}}。    \n",
       "6                 读书使人{{c1::完善}}。    \n",
       "7                  法律体系需要{{c1::完善}}  \n",
       "8               这里的排水设施不{{c1::完善}}。  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Foreign</th>\n      <th>Word</th>\n      <th>Hint</th>\n      <th>Cloze</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>This kind of lever scale is very exact and is ...</td>\n      <td>这杆秤非常精准，买菜的人都用它。</td>\n      <td>精准</td>\n      <td>骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...</td>\n      <td>这杆秤非常{{c1::精准}}，买菜的人都用它。</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Accurate estimates of the marginal costs of pr...</td>\n      <td>精准的边际生产成本的估计是很难达到的。</td>\n      <td>精准</td>\n      <td>骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...</td>\n      <td>{{c1::精准}}的边际生产成本的估计是很难达到的。</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>To obtain accurate results, you need to employ...</td>\n      <td>你需要使用精准的统计工具才能得到正确的结果。</td>\n      <td>精准</td>\n      <td>骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...</td>\n      <td>你需要使用{{c1::精准}}的统计工具才能得到正确的结果。</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>All the chairs matched.</td>\n      <td>所有的椅子都很搭配。</td>\n      <td>搭配</td>\n      <td>骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...</td>\n      <td>所有的椅子都很{{c1::搭配}}。</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The turkey was trimmed with parsley.</td>\n      <td>这盘火鸡用欧芹来搭配点缀。</td>\n      <td>搭配</td>\n      <td>骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...</td>\n      <td>这盘火鸡用欧芹来{{c1::搭配}}点缀。</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>A wide-brimmed hat will be  becoming with that...</td>\n      <td>宽沿帽和她的毛衣更搭配。</td>\n      <td>搭配</td>\n      <td>骚扰, 气不忿儿, 煽动者, 初衷, 节约, 助力, 推动, 制止, 歉意, 周全, 完善,...</td>\n      <td>宽沿帽和她的毛衣更{{c1::搭配}}。</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Reading makes a full man.</td>\n      <td>读书使人完善。</td>\n      <td>完善</td>\n      <td>NaN</td>\n      <td>读书使人{{c1::完善}}。</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The legal system needs to be improved.</td>\n      <td>法律体系需要完善</td>\n      <td>完善</td>\n      <td>NaN</td>\n      <td>法律体系需要{{c1::完善}}</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Drainage here is imperfect.</td>\n      <td>这里的排水设施不完善。</td>\n      <td>完善</td>\n      <td>NaN</td>\n      <td>这里的排水设施不{{c1::完善}}。</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "bs = pd.concat([blah, sent]).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in bs.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bs['Cloze']\n",
    "english = bs['English'].to_list()\n",
    "tip = bs['Hint'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32           这杆秤非常{{c1::精准}}，买菜的人都用它。  \n",
       "33        {{c1::精准}}的边际生产成本的估计是很难达到的。  \n",
       "34     你需要使用{{c1::精准}}的统计工具才能得到正确的结果。  \n",
       "35                 所有的椅子都很{{c1::搭配}}。  \n",
       "36              这盘火鸡用欧芹来{{c1::搭配}}点缀。  \n",
       "37               宽沿帽和她的毛衣更{{c1::搭配}}。  \n",
       "240                   读书使人{{c1::完善}}。  \n",
       "241                    法律体系需要{{c1::完善}}\n",
       "242                 这里的排水设施不{{c1::完善}}。\n",
       "Name: Cloze, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.to_excel('Databases\\BULLSHIT.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "cannot create note because it is a duplicate",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-18336d6c90c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maddCloze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-091aeca585a5>\u001b[0m in \u001b[0;36maddCloze\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Loop through all the rows and create a note for each one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         invoke('addNote', note = {\n\u001b[0m\u001b[0;32m     43\u001b[0m                     \u001b[1;34m\"deckName\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;34m\"modelName\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'(NEW) {} Cloze'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-091aeca585a5>\u001b[0m in \u001b[0;36minvoke\u001b[1;34m(action, **params)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'response is missing required result field'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: cannot create note because it is a duplicate"
     ]
    }
   ],
   "source": [
    "addCloze(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}