{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Scraping Definitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有道词典 definitions \n",
    "defs = driver.find_element_by_xpath('//*[@id=\"phrsListTab\"]/div[2]/ul/p').text\n",
    "# LINE Dict. definitions\n",
    "defs = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[3]/ol').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import re\n",
    "from io import StringIO\n",
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import playsound\n",
    "from numpy import random\n",
    "from datetime import datetime\n",
    "from langdetect import detect\n",
    "import winsound\n",
    "from playsound import playsound\n",
    "from gtts import gTTS\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "source": [
    "## How Do I Study Today?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('what_do.txt', mode = 'r')\n",
    "what_do = file.read()\n",
    "file.close()\n",
    "if what_do == 'listening':\n",
    "    print(\"Listening Today\")\n",
    "    f = open('what_do.txt', 'w+')\n",
    "    f.write('reading')\n",
    "    f.close()\n",
    "elif what_do == 'reading':\n",
    "    print(\"Reading Today\")\n",
    "    f = open('what_do.txt', 'w+')\n",
    "    f.write('listening')\n",
    "    f.close()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## What Words Do I Use to Write Sentences?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDate = datetime.now().date()\n",
    "# Read the Chinese Databases into DataFrames\n",
    "chineseWordsDF = pd.read_excel(r'Databases\\chinese_words.xlsx', sheet_name = 'words', header = 0)\n",
    "chineseWrite = chineseWordsDF.where(chineseWordsDF['DATE'] == sentenceDate - timedelta(days = 3))\n",
    "# Read the Korean Databases into DataFrames\n",
    "koreanWordsDF = pd.read_excel(r'Databases\\korean_words.xlsx', sheet_name = 'words', header = 0)\n",
    "koreanWrite = koreanWordsDF.where(koreanWordsDF['DATE'] == sentenceDate - timedelta(days = 3))\n",
    "# Read the Japanese Databases into DataFrames\n",
    "japaneseWordsDF = pd.read_excel(r'Databases\\japanese_words.xlsx', sheet_name = 'words', header = 0)\n",
    "japaneseWrite = japaneseWordsDF.where(japaneseWordsDF['DATE'] == sentenceDate - timedelta(days = 3))\n",
    "print(\"--------中文-------- \\n\", chineseWrite, \"\\n\")\n",
    "print(\"--------한국어-------- \\n\", koreanWrite, \"\\n\")\n",
    "print(\"--------日本語-------- \\n\", japaneseWrite, \"\\n\")"
   ]
  },
  {
   "source": [
    "## The Importer + Scraper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the current date and time\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "startTime1 = time.time()\n",
    "# Define the three functions we'll be using\n",
    "# Define Vocab Notes function\n",
    "def addVocab(language):\n",
    "    n = 0\n",
    "    # Handle the unique 日本語\n",
    "    if language == '日本語':\n",
    "    # Loop through all the rows and create a note for each one\n",
    "        for index, vocab in enumerate(vocabForeign):\n",
    "            invoke('addNote', note = {\n",
    "                        \"deckName\": '{}'.format(language),\n",
    "                        \"modelName\": '(NEW) {}'.format(language),\n",
    "                        \"fields\": {\n",
    "                            \"(Word) Foreign\": vocab, \n",
    "                            \"(Word) English\": vocabEnglish[index],\n",
    "                            \"(Word) Foreign [漢字]\": kanji\n",
    "                            }  \n",
    "                })\n",
    "            n += 1\n",
    "            print(\"{} has elapsed for {} Vocab words of {} ({} remaining).\".format(convert(time.time() - startTime1), n, words.shape[0], words.shape[0] - n))\n",
    "    # For 한국어 and 中文\n",
    "    else:\n",
    "        # Loop through all the rows and create a note for each one\n",
    "        for index, vocab in enumerate(vocabForeign):\n",
    "            invoke('addNote', note = {\n",
    "                        \"deckName\": '{}'.format(language),\n",
    "                        \"modelName\": '(NEW) {}'.format(language),\n",
    "                        \"fields\": {\n",
    "                            \"(Word) Foreign\": vocab, \n",
    "                            \"(Word) English\": vocabEnglish[index]\n",
    "                            }  \n",
    "                })\n",
    "            n += 1\n",
    "            print(\"{} has elapsed for {} Vocab words of {} ({} remaining).\".format(convert(time.time() - startTime1), n, words.shape[0], words.shape[0] - n))\n",
    "# Define Cloze Notes function\n",
    "def addCloze(language):\n",
    "    n = 0\n",
    "    # Loop through all the rows and create a note for each one\n",
    "    for index, sentence in enumerate(cloze):\n",
    "        invoke('addNote', note = {\n",
    "                    \"deckName\": '{}'.format(language),\n",
    "                    \"modelName\": '(NEW) {} Cloze'.format(language),\n",
    "                    \"fields\": {\n",
    "                        \"Text\": sentence, \n",
    "                        \"Tip\": tip[index],\n",
    "                        \"English\": english[index],\n",
    "                        \"full setence\": fullSentence[index]\n",
    "                        }  \n",
    "            })\n",
    "        n += 1\n",
    "        print(\"{} has elapsed for {} Cloze notes of {} ({} remaining).\".format(convert(time.time() - startTime1), n, dfm.shape[0], dfm.shape[0] - n))\n",
    "\n",
    "# Define the Sentence Scraping function\n",
    "\n",
    "def scrapeSentences(language):\n",
    "    words = vocab['Foreign']\n",
    "    n = 0\n",
    "    o = words.shape[0]\n",
    "    # Create master DataFrame\n",
    "    w = {}\n",
    "    concatList = []\n",
    "    # Run the scraper based on the language\n",
    "    if language == '日本語':\n",
    "        # Not done\n",
    "        # See CSV_Gen [ALL].ipynb\n",
    "        pass\n",
    "    elif language == '中文':\n",
    "        # Define URL for Chinese words\n",
    "        url_zh = 'https://dict.naver.com/linedict/zhendict/dict.html#/cnen/home'\n",
    "        # Define the driver for Selenium to use\n",
    "        driver = webdriver.Chrome('chromedriver.exe')\n",
    "        # Open the specfied URL\n",
    "        driver.get(url_zh)\n",
    "        # Find the radio group and select \"Examples\"\n",
    "        examples_radio = driver.find_element_by_xpath('//*[@id=\"ac_form\"]/fieldset/ul/li[2]/span').click()\n",
    "        # Find the Search bar\n",
    "        search = driver.find_element_by_xpath('//*[@id=\"ac_input\"]')\n",
    "        # Scrape sentences for each word present in the vocabulary list\n",
    "        for word in words:\n",
    "            # Type the word into the Search Box\n",
    "            search.send_keys(word)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(3)\n",
    "            # Find the sentences examples\n",
    "            examples = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]')\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Define an object with all the example sentences\n",
    "            unfiltered_sentences = examples.text\n",
    "            # Clean the string of text that was just scraped\n",
    "            pass1 = unfiltered_sentences.replace(\"more\", \"\")\n",
    "            # Finalize the DataFrame\n",
    "            myList = pass1.split(\"\\n\")\n",
    "            myList = myList[::2]\n",
    "            # Drop the 拼音 be creating a dictionary\n",
    "            myDict = dict(zip(myList[::2], myList[1::2]))\n",
    "            foreign = myDict.keys()\n",
    "            english = myDict.values()\n",
    "            df = pd.DataFrame(columns = ['English', 'Foreign', 'Word', 'Hint', 'Cloze'])\n",
    "            df['English'] = english\n",
    "            df['Foreign'] = foreign\n",
    "            df['Word'] = word\n",
    "            wordsClean = words.tolist()\n",
    "            wordsClean = str(wordsClean).strip(\"[]\")\n",
    "            wordsClean = wordsClean.replace(\"'\", \"\")\n",
    "            df['Hint'] = wordsClean\n",
    "            df['Hint'] = df['Hint'].str.strip('[]')\n",
    "            df['Hint'] = df['Hint'].str.replace(\"'\", \"\")\n",
    "            # Create the Cloze sentences\n",
    "            df['Cloze'] = df['Foreign'].str.replace(word, '{{c1::' + word + '}}')\n",
    "            # Remove examples that don't contain a period\n",
    "            df['English'] = df['English'].map(str)\n",
    "            df1 = df[df['English'].str.contains('\\.', na = False)]\n",
    "            # Reduce the DataFrame to only 3 sentences\n",
    "            df2 = df1.iloc[:3]\n",
    "            # Create the final DataFrame for the word\n",
    "            w[word] = df2\n",
    "            # Add the DataFrame to the list of all word DataFrames\n",
    "            concatList.append(w[word])\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(0.5)\n",
    "            n += 1\n",
    "            o -= 1\n",
    "            print(\"Elapsed time: {} for {} words ({} remaining)\".format(convert(time.time() - startTime1), n, o))\n",
    "            # Exit the FOR loop\n",
    "        # Quit the browser\n",
    "        driver.quit()\n",
    "        # Concatenate each word's DataFrame to the final one with all the sentences\n",
    "        dfm = pd.concat(concatList, ignore_index = True)\n",
    "        # Fix formating issues\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('Name:.*', '')\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('\\n', ',')\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('\\d', '')\n",
    "        # return the created DataFrame\n",
    "        return dfm\n",
    "    elif language == '한국어':\n",
    "        # Define URL for Korean words\n",
    "        url_ko = 'https://en.dict.naver.com/#/search?range=example&shouldSearchVlive=false&query=%ED%9B%84%ED%9A%8C&exampleLevel=exist:1&haveTrans=exist:1'\n",
    "        # Define the driver for Selenium to use\n",
    "        driver = webdriver.Chrome('chromedriver.exe')\n",
    "        # Open the specfied URL\n",
    "        driver.get(url_ko)\n",
    "        # Scrape sentences for each word present in the vocabulary list\n",
    "        for word in words:\n",
    "            # Find the Search bar\n",
    "            search = driver.find_element_by_xpath('//*[@id=\"ac_input\"]')\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Type the word into the Search Box\n",
    "            search.clear()\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            search.send_keys(word)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(3)\n",
    "            # Find the button titled \"Examples\" and click it\n",
    "            example_button = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[1]/div/div/a[4]')\n",
    "            example_button.send_keys(Keys.RETURN)\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Find the \"Level 1\" radio button and select it\n",
    "            level_1 = driver.find_element_by_xpath('//*[@id=\"searchPage_example\"]/div[1]/div/span[2]').click()\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(1)\n",
    "            # Find the \"Have Translations\" check box and check it\n",
    "            have_translations = driver.find_element_by_xpath('//*[@id=\"searchPage_example\"]/div[1]/div/span[5]/label').click()\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2.5)\n",
    "            # Find the \"View more examples\" link and click it if it is displayed\n",
    "            if driver.find_element_by_xpath('//*[@id=\"searchPage_example_more\"]').is_displayed():\n",
    "                driver.find_element_by_xpath('//*[@id=\"searchPage_example_more\"]').click()\n",
    "            else:\n",
    "                pass\n",
    "            # Sleep to avoid errors\n",
    "            time.sleep(2)\n",
    "            # Find the sentence examples\n",
    "            examples = driver.find_element_by_xpath('//*[@id=\"searchPage_example\"]/div[2]')\n",
    "            # Define an object with all the example sentences\n",
    "            unfiltered_sentences = examples.text\n",
    "            pass1 = unfiltered_sentences.replace(\"발음듣기\", \"\")\n",
    "            pass2 = pass1.replace(\"Oxford Advanced Learner's English-Korean Dictionary\", \"\")\n",
    "            pass3 = pass2.replace(\"절대어휘 5100\", \"\")\n",
    "            pass4 = pass3.replace(\"국제영어대학원대학교 신어사전\", \"\")\n",
    "            pass5 = pass4.replace(\"DARAKWON\", \"\")\n",
    "            pass6 = pass5.replace(\"Friendict Level English-Korean Dictionary\", \"\")\n",
    "            pass7 = pass6.replace(\"Web Collection\", \"\")\n",
    "            pass8 = pass7.replace(\"YBM All in All English-Korean Dictionary\", \"\")\n",
    "            pass9 = pass8.replace(\"Neungyule\", \"\")\n",
    "            pass10 = pass9.replace(\"User translation\", \"\")\n",
    "            pass11 = pass10.replace(\"Hansard\", \"\")\n",
    "            pass12 = pass11.replace(\"영어영작문대사전\", \"\")\n",
    "            pass13 = pass12.replace(\"English Hidden Card\", \"\")\n",
    "            pass14 = pass13.replace(\"TIMES CORE\", \"\")\n",
    "            # Create a Dictionary with which to create the final DataFrame\n",
    "            myList = pass14.split(\"\\n\\n\")\n",
    "            # Remove the parentheitcal words from the list\n",
    "            myList2 = []\n",
    "            for i in myList:\n",
    "                i = re.sub(r'\\([^)]*\\)', '', i)\n",
    "                i = i.lstrip()\n",
    "                i = i.rstrip(\"\\n\")\n",
    "                myList2.append(i)\n",
    "            # Make dictionary from the new list\n",
    "            myDict = dict(zip(myList2[::2], myList2[1::2]))\n",
    "            english = myDict.keys()\n",
    "            foreign = myDict.values()\n",
    "            # Create a DataFrame from the scraped string\n",
    "            df = pd.DataFrame(columns = ['English', 'Foreign', 'Word', 'Hint', 'Cloze'])\n",
    "            df['English'] = english\n",
    "            df['Foreign'] = foreign\n",
    "            df['Word'] = word\n",
    "            wordsClean = words.tolist()\n",
    "            wordsClean = str(wordsClean).strip(\"[]\")\n",
    "            wordsClean = wordsClean.replace(\"'\", \"\")\n",
    "            df['Hint'] = wordsClean\n",
    "            df['Hint'] = df['Hint'].str.replace(\"'\", \"\")\n",
    "            # Handle conjugations\n",
    "            if \"하다\" in word:\n",
    "                wordConj = word[:-2]\n",
    "            elif \"다\" in word:\n",
    "                wordConj = word[:-1]\n",
    "            else:\n",
    "                wordConj = word\n",
    "            # Create the Cloze sentences    \n",
    "            df['Cloze'] = df['Foreign'].str.replace(wordConj, '{{c1::' + wordConj + '}}')\n",
    "            # Remove examples that don't contain a period\n",
    "            df['English'] = df['English'].map(str)\n",
    "            # Reduce the DataFrame to only 3 sentences\n",
    "            df2 = df.iloc[:3]\n",
    "            # Create the final DataFrame for the word\n",
    "            w[word] = df2\n",
    "            # Add the DataFrame to the list of all word DataFrames\n",
    "            concatList.append(w[word])\n",
    "            n += 1\n",
    "            o -= 1\n",
    "            print(\"Elapsed time: {} for {} words ({} remaining)\".format(convert(time.time() - startTime1), n, o))\n",
    "            # Exit the FOR loop\n",
    "        # Quit the browser\n",
    "        driver.quit()\n",
    "        # Concatenate each word's DataFrame to the final one with all the sentences\n",
    "        dfm = pd.concat(concatList, ignore_index = True)\n",
    "        # Remove the pesky fuckers that got through the cleaning passes filters\n",
    "        dfm['Foreign'] = dfm['Foreign'].str.replace('\\n.*', '')\n",
    "        dfm['Cloze'] = dfm['Cloze'].str.replace('\\n.*', '')\n",
    "        # return the created DataFrame\n",
    "        return dfm\n",
    "        # Exit the IF-ELIF statement\n",
    "# DEFINE FUNCTIONS\n",
    "\n",
    "def convert(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%dh %02dm %02ds\" % (hour, minutes, seconds)\n",
    "\n",
    "def convertTTS(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%d hours %02d minutes %02d seconds\" % (hour, minutes, seconds)\n",
    "\n",
    "# Define the Anki-Connect functions\n",
    "def request(action, **params):\n",
    "    return {'action': action, 'params': params, 'version': 6}\n",
    "def invoke(action, **params):\n",
    "    requestJson = json.dumps(request(action, **params)).encode('utf-8')\n",
    "    response = json.load(urllib.request.urlopen(urllib.request.Request('http://localhost:8765', requestJson)))\n",
    "    if len(response) != 2:\n",
    "        raise Exception('response has an unexpected number of fields')\n",
    "    if 'error' not in response:\n",
    "        raise Exception('response is missing required error field')\n",
    "    if 'result' not in response:\n",
    "        raise Exception('response is missing required result field')\n",
    "    if response['error'] is not None:\n",
    "        raise Exception(response['error'])\n",
    "    return response['result']\n",
    "\n",
    "# THE REAL WORK BEGINS\n",
    "\n",
    "# Read CSVs into DataFrames\n",
    "vocab = pd.read_excel(r'Flashcards.xlsx', sheet_name = 'vocab', header = 0)\n",
    "vocab.columns = ['Foreign', 'English', 'Kanji', 'Language']\n",
    "vocab['English'] = vocab['English'].replace('s/\\([^)]*\\)//', '')\n",
    "# Retrieve the language from the vocab DataFrame then drop its column\n",
    "language = vocab['Language'].loc[0:0].to_string(index = False, header = False)\n",
    "vocab.drop(['Language'], inplace = True, axis = 1)\n",
    "# Strip the leading space from the lang variable (Thank you shit Excel dropdown menu! /s)\n",
    "language = language.lstrip()\n",
    "# Format English Definitions \n",
    "if language == '中文':\n",
    "    # CHINESE DEFINIIIONS FORMATTING\n",
    "    # Add spaces after Numbers\n",
    "    vocab['English'] = vocab['English'].replace(\"([1-9])\", \"\\\\1.) \", regex = True)\n",
    "    # Remove empty paragraphs\n",
    "    vocab['English'] = vocab['English'].replace('\\n\\n', '; ', regex = True)\n",
    "    # Remove spaces before semicolons\n",
    "    vocab['English'] = vocab['English'].replace(' ;', ',', regex = True)\n",
    "    # Add a new line after semicolons\n",
    "    vocab['English'] = vocab['English'].replace('(;)', '\\\\1\\n', regex = True)\n",
    "elif language == '한국어':\n",
    "    # KOREAN DEFINITIONS FORMATTING\n",
    "    # Remove empty paragraphs\n",
    "    vocab['English'] = vocab['English'].replace('\\n', ' ', regex = True)\n",
    "    # Add a new line before a number\n",
    "    vocab['English'] = vocab['English'].replace(\"([2-9])\", \"\\n \\\\1\", regex = True)\n",
    "elif language == '日本語':\n",
    "    pass\n",
    "# Define the returned DataFrame from scrapeSentences()\n",
    "words = vocab['Foreign']\n",
    "print(\"Processing {} {} words for scraping...\".format(words.shape[0], language))\n",
    "dfm = scrapeSentences(language)\n",
    "# Define the necessary variables\n",
    "vocabForeign = vocab['Foreign'].to_list()\n",
    "vocabEnglish = vocab['English'].to_list()\n",
    "kanji = vocab['Kanji'].to_list()\n",
    "# Define the necessary variables\n",
    "cloze = dfm['Cloze'].to_list()\n",
    "tip = dfm['Hint'].to_list()\n",
    "english = dfm['English'].to_list()\n",
    "fullSentence = dfm['Foreign'].to_list()\n",
    "# Add the Notes\n",
    "print(\"Processing {} {} Vocabulary...\".format(words.shape[0], language))\n",
    "addVocab(language)\n",
    "print(\"Processing {} {} Clozes...\".format(dfm.shape[0], language))\n",
    "addCloze(language)\n",
    "if language == '中文':\n",
    "    # Read the Chinese Databases into DataFrames\n",
    "    chineseWordsDF = pd.read_excel(r'Databases\\chinese_words.xlsx', sheet_name = 'words', header = 0)\n",
    "    chineseSentencesDF = pd.read_excel(r'Databases\\chinese_sentences.xlsx', sheet_name = 'sentences', header = 0)\n",
    "elif language == '한국어':\n",
    "    # Read the Korean Databases into DataFrames\n",
    "    koreanWordsDF = pd.read_excel(r'Databases\\korean_words.xlsx', sheet_name = 'words', header = 0)\n",
    "    koreanSentencesDF = pd.read_excel(r'Databases\\korean_sentences.xlsx', sheet_name = 'sentences', header = 0)\n",
    "elif language == '日本語':\n",
    "    # Read the Japanese Databases into DataFrames\n",
    "    japaneseWordsDF = pd.read_excel(r'Databases\\japanese_words.xlsx', sheet_name = 'words', header = 0)\n",
    "    japaneseSentencesDF = pd.read_excel(r'Databases\\japanese_sentences.xlsx', sheet_name = 'sentences', header = 0)\n",
    "# Get today's date and store it as an object\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# Format the DataFrame to be appended to our local Database\n",
    "dfmAppend = dfm.drop(axis = 1, columns = ['Hint', 'Cloze'])\n",
    "dfmAppend.columns = ['ENGLISH', 'FOREIGN', 'WORD']\n",
    "dfmAppend['DATE'] = date\n",
    "if language != '日本語':\n",
    "    vocabAppend = vocab.drop(axis = 1, columns = ['Kanji'])\n",
    "    vocabAppend.columns = ['FOREIGN', 'ENGLISH']\n",
    "else:\n",
    "    vocabAppend = vocab\n",
    "    vocabAppend.columns = ['FOREIGN', 'ENGLISH', 'KANJI']\n",
    "vocabAppend['DATE'] = date\n",
    "# Add the Vocabulary to the appropriate Database\n",
    "if language == '中文':\n",
    "    chineseWordsDF = chineseWordsDF.append(vocabAppend)\n",
    "    chineseSentencesDF = chineseSentencesDF.append(dfmAppend)\n",
    "    # Overwrite the old Databases with the new one\n",
    "    chineseWordsDF.to_excel('Databases\\chinese_words.xlsx', sheet_name = 'words', index = False)\n",
    "    chineseSentencesDF.to_excel('Databases\\chinese_sentences.xlsx', sheet_name = 'sentences', index = False)\n",
    "elif language == '한국어':\n",
    "    koreanWordsDF = koreanWordsDF.append(vocabAppend)\n",
    "    koreanSentencesDF = koreanSentencesDF.append(dfmAppend)\n",
    "    # Overwrite the old Databases with the new one\n",
    "    koreanWordsDF.to_excel('Databases\\korean_words.xlsx', sheet_name = 'words', index = False)\n",
    "    koreanSentencesDF.to_excel('Databases\\korean_sentences.xlsx', sheet_name = 'sentences', index = False)\n",
    "elif language == '日本語':\n",
    "    japaneseWordsDF = japaneseWordsDF.append(vocabAppend)\n",
    "    japaneseSentencesDF = japaneseSentencesDF.append(dfmAppend)\n",
    "    # Overwrite the old Databases with the new one\n",
    "    japaneseWordsDF.to_excel('Databases\\japanese_words.xlsx', sheet_name = 'words', index = False)\n",
    "    japaneseSentencesDF.to_excel('Databases\\japanese_sentences.xlsx', sheet_name = 'sentences', index = False)\n",
    "print(\"ALL PROCESSES COMPLETED! ELAPSED TIME: {}\".format(convert(time.time() - startTime1)))\n",
    "tts = gTTS(text = \"All processes have completed in {}.\".format(convertTTS(time.time() - startTime1)), lang = \"en\")\n",
    "tts.save(\"tts.mp3\")\n",
    "playsound(\"tts.mp3\")\n",
    "os.remove(\"tts.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}